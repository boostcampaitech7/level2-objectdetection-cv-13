{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9614999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import detectron2\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91b14f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Dataset\n",
    "try:\n",
    "    register_coco_instances('coco_trash_test', {}, 'dataset/test.json', 'dataset/')\n",
    "except AssertionError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f7ad5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 불러오기\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file('COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2259060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 수정하기\n",
    "cfg.DATASETS.TEST = ('coco_trash_test',)\n",
    "\n",
    "cfg.DATALOADER.NUM_WOREKRS = 2\n",
    "\n",
    "cfg.OUTPUT_DIR = './output'\n",
    "\n",
    "cfg.MODEL.WEIGHTS = '/data/ephemeral/home/HyeWon/output/model_final.pth'\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 10\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49e6aa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/14 16:16:32 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /data/ephemeral/home/HyeWon/output/model_final.pth ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../dataset/test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data/ephemeral/home/HyeWon/faster_rcnn_inference.ipynb 셀 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.28.224.166/data/ephemeral/home/HyeWon/faster_rcnn_inference.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset_dict\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.28.224.166/data/ephemeral/home/HyeWon/faster_rcnn_inference.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# test loader\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.28.224.166/data/ephemeral/home/HyeWon/faster_rcnn_inference.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m test_loader \u001b[39m=\u001b[39m build_detection_test_loader(cfg, \u001b[39m'\u001b[39;49m\u001b[39mcoco_trash_test\u001b[39;49m\u001b[39m'\u001b[39;49m, MyMapper)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/config/config.py:207\u001b[0m, in \u001b[0;36mconfigurable.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(orig_func)\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m _called_with_cfg(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 207\u001b[0m         explicit_args \u001b[39m=\u001b[39m _get_args_from_config(from_config, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    208\u001b[0m         \u001b[39mreturn\u001b[39;00m orig_func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexplicit_args)\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/config/config.py:245\u001b[0m, in \u001b[0;36m_get_args_from_config\u001b[0;34m(from_config_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m supported_arg_names:\n\u001b[1;32m    244\u001b[0m         extra_kwargs[name] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(name)\n\u001b[0;32m--> 245\u001b[0m ret \u001b[39m=\u001b[39m from_config_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    246\u001b[0m \u001b[39m# forward the other arguments to __init__\u001b[39;00m\n\u001b[1;32m    247\u001b[0m ret\u001b[39m.\u001b[39mupdate(extra_kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/data/build.py:562\u001b[0m, in \u001b[0;36m_test_loader_from_config\u001b[0;34m(cfg, dataset_name, mapper)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset_name, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    560\u001b[0m     dataset_name \u001b[39m=\u001b[39m [dataset_name]\n\u001b[0;32m--> 562\u001b[0m dataset \u001b[39m=\u001b[39m get_detection_dataset_dicts(\n\u001b[1;32m    563\u001b[0m     dataset_name,\n\u001b[1;32m    564\u001b[0m     filter_empty\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    565\u001b[0m     proposal_files\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    566\u001b[0m         cfg\u001b[39m.\u001b[39;49mDATASETS\u001b[39m.\u001b[39;49mPROPOSAL_FILES_TEST[\u001b[39mlist\u001b[39;49m(cfg\u001b[39m.\u001b[39;49mDATASETS\u001b[39m.\u001b[39;49mTEST)\u001b[39m.\u001b[39;49mindex(x)] \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m dataset_name\n\u001b[1;32m    567\u001b[0m     ]\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;49;00m cfg\u001b[39m.\u001b[39;49mMODEL\u001b[39m.\u001b[39;49mLOAD_PROPOSALS\n\u001b[1;32m    569\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[1;32m    571\u001b[0m \u001b[39mif\u001b[39;00m mapper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     mapper \u001b[39m=\u001b[39m DatasetMapper(cfg, \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/data/build.py:253\u001b[0m, in \u001b[0;36mget_detection_dataset_dicts\u001b[0;34m(names, filter_empty, min_keypoints, proposal_files, check_consistency)\u001b[0m\n\u001b[1;32m    246\u001b[0m     logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    247\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe following dataset names are not registered in the DatasetCatalog: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnames_set\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mavailable_datasets\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAvailable datasets are \u001b[39m\u001b[39m{\u001b[39;00mavailable_datasets\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 253\u001b[0m dataset_dicts \u001b[39m=\u001b[39m [DatasetCatalog\u001b[39m.\u001b[39mget(dataset_name) \u001b[39mfor\u001b[39;00m dataset_name \u001b[39min\u001b[39;00m names]\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset_dicts[\u001b[39m0\u001b[39m], torchdata\u001b[39m.\u001b[39mDataset):\n\u001b[1;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dataset_dicts) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    257\u001b[0m         \u001b[39m# ConcatDataset does not work for iterable style dataset.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m         \u001b[39m# We could support concat for iterable as well, but it's often\u001b[39;00m\n\u001b[1;32m    259\u001b[0m         \u001b[39m# not a good idea to concat iterables anyway.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/data/build.py:253\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    246\u001b[0m     logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    247\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe following dataset names are not registered in the DatasetCatalog: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnames_set\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mavailable_datasets\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAvailable datasets are \u001b[39m\u001b[39m{\u001b[39;00mavailable_datasets\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 253\u001b[0m dataset_dicts \u001b[39m=\u001b[39m [DatasetCatalog\u001b[39m.\u001b[39;49mget(dataset_name) \u001b[39mfor\u001b[39;00m dataset_name \u001b[39min\u001b[39;00m names]\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset_dicts[\u001b[39m0\u001b[39m], torchdata\u001b[39m.\u001b[39mDataset):\n\u001b[1;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dataset_dicts) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    257\u001b[0m         \u001b[39m# ConcatDataset does not work for iterable style dataset.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m         \u001b[39m# We could support concat for iterable as well, but it's often\u001b[39;00m\n\u001b[1;32m    259\u001b[0m         \u001b[39m# not a good idea to concat iterables anyway.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/data/catalog.py:58\u001b[0m, in \u001b[0;36m_DatasetCatalog.get\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     53\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataset \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not registered! Available datasets are: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     55\u001b[0m             name, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys()))\n\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[39mreturn\u001b[39;00m f()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/data/datasets/coco.py:500\u001b[0m, in \u001b[0;36mregister_coco_instances.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(image_root, (\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike)), image_root\n\u001b[1;32m    499\u001b[0m \u001b[39m# 1. register a function which returns dicts\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m DatasetCatalog\u001b[39m.\u001b[39mregister(name, \u001b[39mlambda\u001b[39;00m: load_coco_json(json_file, image_root, name))\n\u001b[1;32m    502\u001b[0m \u001b[39m# 2. Optionally, add metadata about this dataset,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39m# since they might be useful in evaluation, visualization or logging\u001b[39;00m\n\u001b[1;32m    504\u001b[0m MetadataCatalog\u001b[39m.\u001b[39mget(name)\u001b[39m.\u001b[39mset(\n\u001b[1;32m    505\u001b[0m     json_file\u001b[39m=\u001b[39mjson_file, image_root\u001b[39m=\u001b[39mimage_root, evaluator_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcoco\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmetadata\n\u001b[1;32m    506\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/detectron2/data/datasets/coco.py:69\u001b[0m, in \u001b[0;36mload_coco_json\u001b[0;34m(json_file, image_root, dataset_name, extra_annotation_keys)\u001b[0m\n\u001b[1;32m     67\u001b[0m json_file \u001b[39m=\u001b[39m PathManager\u001b[39m.\u001b[39mget_local_path(json_file)\n\u001b[1;32m     68\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(io\u001b[39m.\u001b[39mStringIO()):\n\u001b[0;32m---> 69\u001b[0m     coco_api \u001b[39m=\u001b[39m COCO(json_file)\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m timer\u001b[39m.\u001b[39mseconds() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     71\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m takes \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m seconds.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(json_file, timer\u001b[39m.\u001b[39mseconds()))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pycocotools/coco.py:81\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloading annotations into memory...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m tic \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 81\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(annotation_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     82\u001b[0m     dataset \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m     83\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(dataset)\u001b[39m==\u001b[39m\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mannotation file format \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not supported\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(dataset))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../dataset/test.json'"
     ]
    }
   ],
   "source": [
    "# model\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# mapper - input data를 어떤 형식으로 return할지\n",
    "def MyMapper(dataset_dict):\n",
    "    \n",
    "    dataset_dict = copy.deepcopy(dataset_dict)\n",
    "    image = utils.read_image(dataset_dict['file_name'], format='BGR')\n",
    "    \n",
    "    dataset_dict['image'] = image\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "# test loader\n",
    "test_loader = build_detection_test_loader(cfg, 'coco_trash_test', MyMapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da993da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output 뽑은 후 sumbmission 양식에 맞게 후처리 \n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "\n",
    "class_num = 10\n",
    "\n",
    "for data in tqdm(test_loader):\n",
    "    \n",
    "    prediction_string = ''\n",
    "    \n",
    "    data = data[0]\n",
    "    \n",
    "    outputs = predictor(data['image'])['instances']\n",
    "    \n",
    "    targets = outputs.pred_classes.cpu().tolist()\n",
    "    boxes = [i.cpu().detach().numpy() for i in outputs.pred_boxes]\n",
    "    scores = outputs.scores.cpu().tolist()\n",
    "    \n",
    "    for target, box, score in zip(targets,boxes,scores):\n",
    "        prediction_string += (str(target) + ' ' + str(score) + ' ' + str(box[0]) + ' ' \n",
    "        + str(box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ' ')\n",
    "    \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(data['file_name'].replace('../../dataset/',''))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.OUTPUT_DIR, f'submission_det2.csv'), index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
